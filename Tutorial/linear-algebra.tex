\chapter{Vectors and Matrices \label{chapter:linear-algebra.tex}}
Certain applications, for example data mining and machine learning, require an efficient support of both
vectors and matrices. 
Although vectors and matrices could easily be implemented in \setlx, this would not be very
efficient because of the overhead of the interpreter loop.  Therefore, \setlx\ supports both vectors and
matrices natively.  This support is based on the \textsl{Java} library 
\href{http://math.nist.gov/javanumerics/jama/}{\textsl{Jama}}.  This library is the result of a joint effort of
\href{http://www.mathworks.com/}{Mathworks} and the \href{https://www.nist.gov}{National Institute
  of Standards and Technology}.  \textsl{Jama} has been integrated into  
\setlx\ by Patrick Robinson.  This library provides the basic means for computations involving
matrices and vectors.  In the following exposition we assume that the reader has some familiarity with  
\href{http://en.wikipedia.org/wiki/Linear_algebra}{linear algebra}.

\section{Vectors}
\setlx\ supports real valued vectors of arbitrary dimensions.  Conceptually, a vector can be viewed as
a list of floating point numbers.  A vector is constructed from a list of numbers via the
function \texttt{la\_vector} as follows:
\\[0.2cm]
\hspace*{1.3cm}
\texttt{v := la\_vector([1/2, 1/4, 1/5]);}
\\[0.2cm]
When executed, this statement yields the result
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<<0.5 0.25 0.2>>}.
\\[0.2cm]
This result shows that the fractions in the argument list have been converted to floating point
numbers.  Conceptually, the vector \texttt{v} is a column vector.  Therefore, mathematically
\texttt{v} would be written as
\\[0.2cm]
\hspace*{1.3cm}
$
\left(\begin{array}{l}
  0.5  \\
  0.25 \\
  0.2
\end{array}\right)
$.
\\[0.2cm]
Instead of using the function \texttt{la\_vector,} we could also have used the command
\\[0.2cm]
\hspace*{1.3cm}
\texttt{v := <<1/2 1/4 1/5>>;}
\\[0.2cm]
to define the vector \texttt{v}.  This shows that the operators ``\texttt{<<}'' and ``\texttt{>>}''
can be used to define a vector.  \textbf{Note} that the components of a vector are \textbf{not} separated by the
character ``\texttt{,}'' but \textbf{rather} are separated by white space.

\setlx\ support the basic arithmetic operations that are defined for vectors.  If \texttt{a} and
\texttt{b} are two vectors, then 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a + b}
\\[0.2cm]
computes the sum of \texttt{a} and \texttt{b}, while 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a - b}
\\[0.2cm]
computes their difference.  Concretely, if we have
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a = <<a$_1$ $\cdots$ a$_n$>>} \quad and \quad
\texttt{b = <<b$_1$ $\cdots$ b$_n$>>},
\\[0.2cm]
then ``\texttt{a+b}'' and ``\texttt{a-b}'' are defined componentwise:
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a + b := <<(a$_1$+b$_1$) $\cdots$ (a$_n$+b$_n$)>>} \quad and \quad
\texttt{a - b := <<(a$_1$-b$_1$) $\cdots$ (a$_n$-b$_n$)>>}.
\\[0.2cm]
For example, if we define
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a := <<1 2 3>>; \ b := <<4 5 6>>;}
\\[0.2cm]
then the expressions ``\texttt{a + b}'' and ``\texttt{a - b}'' yield the results
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<<5.0 7.0 9.0>>} \quad and \quad \texttt{<<-3.0 -3.0 -3.0>>},
\\[0.2cm]
respectively.  Additionally, the shortcut assignment operators ``\texttt{+=}'' and ``\texttt{-=}''
are available for vectors.  They work as expected.  Note that for the expressions ``\texttt{a + b}''
and ``\texttt{a - b}'' to be defined, \texttt{a} and \texttt{b} have to have 
\textbf{the same number of elements}. 

Vectors support 
\href{http://en.wikipedia.org/wiki/Scalar_multiplication}{\emph{scalar multiplication}}.  If
$\alpha$ is a real number and 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{v = <<v$_1$ $\cdots$ a$_n$>>}
\\[0.2cm]
is an $n$-dimensional number, then the scalar products \texttt{$\alpha$ * v} and \texttt{v * $\alpha$}
are both defined as the vector
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<<($\alpha$ * v$_1$) $\cdots$ ($\alpha$ * v$_n$)>>}.
\\[0.2cm]
For example, if \texttt{a} is defined as above, then the expression
\\[0.2cm]
\hspace*{1.3cm}
\texttt{1/2 * a}
\\[0.2cm]
yields the vector
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<<0.5 1.0 1.5>>}.
\\[0.2cm]
It does not matter whether we multiply the scalar from the left or from the right, so the expression
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a * (1/2)}
\\[0.2cm]
yields the same result.  Note that we had to put the fraction \texttt{1/2} in parenthesis.  The
reason is that the expression ``\texttt{a * 1/2}'' is parsed as ``\texttt{(a * 1) / 2}'' and
division of a vector by a scalar is not defined.  If \texttt{v} is a vector and \texttt{n} is a
number, then the assignment statement
\\[0.2cm]
\hspace*{1.3cm}
\texttt{v *= n;}
\\[0.2cm]
is equivalent to the statement
\\[0.2cm]
\hspace*{1.3cm}
\texttt{v = v * n;}
\\[0.2cm]
For two $n$-dimensional vectors
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x} = \left(
  \begin{array}[c]{c}
  x_1 \\
  \vdots \\
  x_n    
  \end{array}
\right)
$ 
\quad and \quad
$\vec{y} = \left(
  \begin{array}[c]{c}
  y_1 \\
  \vdots \\
  y_n    
  \end{array}
\right),
$ 
\\[0.2cm]
the \href{http://en.wikipedia.org/wiki/Dot_product}{\emph{dot product}}, which is also known as the  
\emph{scalar product}, is defined as
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x} \cdot \vec{y} := \sum\limits_{i=1}^n x_i \cdot y_i$.
\\[0.2cm]
Students often confuse scalar multiplication and the scalar product.
Therefore, we have decided to use the same operator for both products:
If \texttt{a} and \texttt{b} are two vectors of the same dimension, the expression
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a * b}
\\[0.2cm]
yields their scalar product.  For example,
using the definition of \texttt{a} and \texttt{b} given above, the expression
``\texttt{a * b}''
yields the result $32$ since
\\[0.2cm]
\hspace*{1.3cm}
$1 \cdot 4 + 2 \cdot 5 + 3 \cdot 6 = 32$.
\\[0.2cm]
Finally, \setlx\ supports the 
\href{http://en.wikipedia.org/wiki/Cross_product}{\emph{cross product}}.  For two three-dimensional
vectors 
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x} = \left(
  \begin{array}[c]{c}
  x_1 \\
  x_2 \\
  x_3    
  \end{array}
\right)
$ 
\quad and \quad
$\vec{y} = \left(
  \begin{array}[c]{c}
  y_1 \\
  y_2 \\
  y_3    
  \end{array}
\right),
$ 
\\[0.2cm]
the cross product $\vec{x} \times \vec{y}$ is defined as
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x} = \left(
  \begin{array}[c]{c}
  x_1 \\
  x_2 \\
  x_3    
  \end{array}
\right) \times \left(
  \begin{array}[c]{c}
  y_1 \\
  y_2 \\
  y_3    
  \end{array}
\right) = \left(
  \begin{array}[c]{c}
  x_2 \cdot y_3 - x_3 \cdot y_2 \\
  x_3 \cdot y_1 - x_1 \cdot y_3 \\
  x_1 \cdot y_2 - x_2 \cdot y_1    
  \end{array}
\right)
$. 
\\[0.2cm]
If \texttt{a} and
\texttt{b} are both 3-dimensional vectors, then the expression
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a >< b}
\\[0.2cm]
computes the cross product of \texttt{a} and \texttt{b}.  For example, if \texttt{a} and \texttt{b}
are the vectors defined at the beginning of this section, then the expression
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a >< b}
\\[0.2cm]
yields the result
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<<-3.0  6.0  -3.0>>}
\\[0.2cm]
since we have
\\[0.2cm]
\hspace*{1.3cm}
$\left(
 \begin{array}[c]{l}
   1 \\ 2 \\ 3
 \end{array}
 \right)
   \times
 \left(
 \begin{array}[c]{l}
    4 \\ 5 \\ 6
 \end{array}
 \right)
    =
 \left(
  \begin{array}[c]{l}
    2 \cdot 6 - 3 \cdot 5 \\ 3 \cdot 4 - 6 \cdot 1 \\ 1 \cdot 5 - 2 \cdot 4
  \end{array}
 \right)
    =
 \left(
  \begin{array}[c]{r}
    -3 \\ 6 \\ -3
  \end{array}
 \right)
$.
\\[0.2cm]
Note that the cross product of two vectors is only defined iff both \texttt{a} and \texttt{b} are 
three-dimensional.  There are generalisations of the cross product for $n$-dimensional vectors.
However, in that case the cross product is no longer a binary operator but rather takes $n-1$ arguments.
These generalisations are not supported in \setlx.

Vectors provide the same access operations as list.  Therefore, to extract the $i$-th component of a
vector \texttt{v} we can use the expression ``\texttt{v[$i$]}''.  Furthermore, the operator
``\texttt{\#}'' returns the dimension of a given vector.  Therefore, given the vector \texttt{a}
defined above, the expression \texttt{\#a} returns the value 3.


\section{Matrices}
\setlx\ supports real valued matrices.  The function \texttt{la\_matrix} can be used to construct a
matrix from a list of list of numbers.  For example, the assignment
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a := la\_matrix([[1,2],[3,4]]);}
\\[0.2cm]
constructs a matrix that can be written as
\\[0.2cm]
\hspace*{1.3cm}
$
\left(
\begin{array}[c]{ll}
  1 & 2 \\
  3 & 4
\end{array}
\right)
$.
\\[0.2cm]
We can see that the inner lists used as an argument to the function \texttt{la\_matrix} correspond
to the rows of the resulting matrix.   In \setlx, the matrix is printed as
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<< <<1.0 2.0>> <<3.0 4.0>> >>}.
\\[0.2cm]
Note that a matrix is written as a list of vectors where instead of the square brackets
``\texttt{[}'' and ``\texttt{]}''  the tokens ``\texttt{<<}'' and
``\texttt{>>}'' are used as opening and closing delimiters.  However, whereas the elements of a list
are separated by commas, the row vectors making up a matrix are separated by white space.  
Instead of using the function \texttt{la\_matrix} we could have defined the matrix \texttt{a} using
the following command:
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a := << <<1.0 2.0>> <<3.0 4.0>> >>;}
\\[0.2cm]  
The function \texttt{la\_matrix} can also be called with a single vector as its argument.  If $v$ is an
$n$-dimensional vector, then 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{la\_matrix($v$)}
\\[0.2cm]
interprets this vector as a column vector and turns it into an $n \times 1$ matrix.  For example, 
the statement 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a := la\_matrix(<<1 2 3>>);}
\\[0.2cm]
yields the result
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<< <<1.0>> <<2.0>> <<3.0>> >>}.
\\[0.2cm]
This result shows that \texttt{a} is a matrix that consists of three rows that are themselves
vectors of length 1.

Similar to vectors, matrices can be added and subtracted using the operators ``\texttt{+}'' and
``\texttt{-}''.   If we define
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a := << <<1 2>> <<3 4>> >>;} \quad and \quad \texttt{b := << <<5 6>> <<7 8>> >>;}   
\\[0.2cm]
then the expressions ``\texttt{a + b}'' and ``\texttt{a - b}'' work componentwise and yield the
results 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<< <<6.0 8.0>> <<10.0 12.0>> >>} \quad and \quad 
\texttt{<< <<-4.0 -4.0>> <<-4.0 -4.0>> >>}.
\\[0.2cm]
In addition to ``\texttt{+}'' and ``\texttt{-}'', the assignment operators ``\texttt{+=}'' and
``\texttt{-=}'' are supported for matrices and work as expected.

Furthermore, matrices support scalar multiplication in the same way as vectors.  For example, if the
matrix \texttt{a} is defined as abobe, then the expressions 
``\texttt{2 * a}'' and ``\texttt{a * 2}'' both yield the result
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<< <<2.0 4.0>> <<6.0 8.0>> >>}.
\vspace*{0.2cm} 

Next,
\setlx\ supports \href{http://en.wikipedia.org/wiki/Matrix_multiplication}{matrix multiplication}.
If
\\[0.2cm]
\hspace*{1.3cm}
 $\texttt{a} = (\mathtt{a}_{i,j})_{1 \leq i \leq m \atop 1 \leq j \leq n}$ \quad is an $m \times n$ matrix 
\\
and
\\
\hspace*{1.3cm}
$\texttt{b} = (\mathtt{b}_{j,l})_{1 \leq j \leq n \atop 1 \leq l \leq k}$ \quad is an $n \times k$ matrix, 
\\[0.2cm]
then the matrix product \texttt{a * b} is defined as the matrix
\\[0.2cm]
\hspace*{1.3cm}
$\Bigl(\sum\limits_{j=1}^n \texttt{a}_{i,j}\cdot \texttt{b}_{j,l}\Bigr)_{1 \leq i \leq n \atop 1 \leq l \leq k}$.
\\[0.2cm]
For example, given the definitions of
\texttt{a} and \texttt{b} shown above,  the expression ``\texttt{a * b}'' yields the result
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<< <<19.0 22.0>> <<43.0 50.0>> >>}.
\\[0.2cm]
Furthermore, if \texttt{a} is an $m \times n$ matrix and \texttt{v} is an $n$ dimensional vector,
then the expression 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a * v}
\\[0.2cm]
is computed as a matrix multiplication, where $v$ is interpreted as an $n \times 1$ matrix.  In
this case, the resulting $m \times 1$ matrix is then automatically converted back into an $m$ dimensional vector.

In addition to matrix multiplication, \setlx\ also support exponentiation of a square matrix by an
integer number.  For example, 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a ** 2;}
\\[0.2cm]
returns the square  of \texttt{a}, while 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a ** -1}
\\[0.2cm]
return the \href{http://en.wikipedia.org/wiki/Invertible_matrix}{\emph{inverse}} of a matrix,
provided the matrix is not \emph{singular}.  If the matrix \texttt{a} is singular, evaluation of the
expression \texttt{a ** $n$} raises an exception if the exponent $n$ is negative.

Matrices can be \href{http://en.wikipedia.org/wiki/Transpose}{\emph{transposed}} via the postfix
operator ``\texttt{!}''.  For example, using the definition of the matrix \texttt{a} shown above,
the expression ``\texttt{a!}'' yields 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<< <<1.0 3.0>> <<2.0 4.0>> >>}.
\\[0.2cm]
If \texttt{a} is an $m \times n$ matrix, the expression ``\texttt{\#a}'' yields the dimension $m$.  
In order to compute the dimension $n$, we can use the expression 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{\#a[1]}.
\\[0.2cm]
The reason is that \texttt{a[1]} returns the first row of the matrix \texttt{a} as a list and the
length of this list is the dimension $n$.  In order to access the element in row $i$ and column $j$
of matrix \texttt{a} we can use the expression
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a[$i$][$j$]}.

\section{Numerical Methods for Matrices and Vectors}
In this section we will discuss the numerical methods that are provided.  These methods are inherited
from \href{http://math.nist.gov/javanumerics/jama/}{\textsl{Jama}}, which is a \textsl{Java} matrix
package.  The names of all methods inherited from \textsl{Jama} start with ``\texttt{la\_}''.  This
is short for \underline{l}inear \underline{a}lgebra.

\subsection{Computing the Determinant}
If \texttt{a} is a square matrix, the expression 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{la\_det(a)}
\\[0.2cm]
computes the \href{http://en.wikipedia.org/wiki/Determinant}{\emph{determinant}} of \texttt{a}.
For example, if we define 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a := << <<1 2>> <<3 4>> >>;}
\\[0.2cm]
then the expression ``\texttt{la\_det(a)}'' yields the result $-2$.  The determinant can be used to
check whether a matrix is invertible as a matrix is invertible if and only if the determinant
is different from $0$.  However, note that due to rounding errors the result of the expression 
``\texttt{la\_det(a)}''  might be a small non-zero number even if the matrix \texttt{a} is really
singular. 

\subsection{Solving a System of Linear Equations}
A system of linear equations of the form
\\[0.2cm]
\hspace*{1.3cm}
$\texttt{a} \cdot \texttt{x} = \texttt{b}$
\\[0.2cm]
where \texttt{a} is a square matrix $n \times n$  matrix and  \texttt{b} is an $n$-dimensional
vector can be solved using the expression
\\[0.2cm]
\hspace*{1.3cm}
\texttt{la\_solve(a, b)}.
\\[0.2cm]
For example, to solve the system of equations
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[c]{lcl}
  1 \cdot x + 2 \cdot y & = & 5 \\[0.1cm]
  3 \cdot x + 4 \cdot y & = & 6 
\end{array}
$
\\[0.2cm]
we define
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a := << <<1 2>> <<3 4>> >>;} \quad and \quad \texttt{b := <<5 6>>;}
\\[0.2cm]
Then, the solution is computed via the expression
\\[0.2cm]
\hspace*{1.3cm}
\texttt{la\_solve(a, b)}.
\\[0.2cm]
Note that this expression throws an exception if the given system of equations is not solvable.

Note that it is also posible to solve a system of equations that is 
\href{http://en.wikipedia.org/wiki/Overdetermined_system}{\emph{overdetermined}}.  For example,
assume that the system of equations is given as
\\[0.2cm]
\hspace*{1.3cm}
$
\begin{array}[c]{lcl}
  1 \cdot x + 2 \cdot y & = & 1 \\[0.1cm]
  2 \cdot x + 3 \cdot y & = & 2 \\[0.1cm]
  3 \cdot x + 4 \cdot y & = & 3 
\end{array}
$
\\[0.2cm]
In order to compute the solution $\vec{x}$ that minimizes the error
 $\|a \cdot \vec{x} - \vec{b}\|_2$ we can 
define \texttt{a} and \texttt{b} via
\\[0.2cm]
\hspace*{1.3cm}
\texttt{a := << <<1 2>> <<2 3>> <<3 4>> >>;} \quad and \quad \texttt{b := <<1 2 3>>;}
\\[0.2cm]
and then call  the function \texttt{la\_solve} as 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{la\_solve(a, b);}
\\[0.2cm]
In this case, \texttt{la\_solve} computes the result
\\[0.2cm]
\hspace*{1.3cm}
\texttt{<<0.9999999999999982 1.4430967688835178E-15>>}
\\[0.2cm]
which is pretty close to the exact result $\left(\begin{array}[c]{l} 1 \\ 0 \end{array}\right)$.
\vspace*{0.3cm}

\noindent
When solving a system of linear equations, care has to be taken that the solution is well defined.
To this end, \setlx\ provides the function \texttt{la\_cond} that computes the 
\href{http://en.wikipedia.org/wiki/Condition_number}{condition number} of a given matrix.
Let us describe the problem via an example borrowed from
\\[0.2cm]
\hspace*{1.3cm}
\href{http://nm.mathforcollege.com/mws/gen/04sle/mws_gen_sle_spe_adequacy.pdf}{\texttt{http://nm.mathforcollege.com/mws/gen/04sle/mws\_gen\_sle\_spe\_adequacy.pdf}}.
\\[0.2cm]
Assume the matrix $a$ and the vector $\vec{b}$ are defined as
\\[0.2cm]
\hspace*{1.3cm}
$a := \left(
  \begin{array}[c]{ll}
    1.000 &  2.000 \\
    2.000 &  3.999
  \end{array}
  \right)
$  \quad and \quad
$\vec{b} := \left(
  \begin{array}[c]{l}
    4.000 \\ 7.999
  \end{array}
  \right).
$
\\[0.2cm]
It can easily be verified that the solution $\vec{x}$ to the system of equations 
\\[0.2cm]
\hspace*{1.3cm}
$\vec{b} = a \cdot \vec{x}$
\\[0.2cm]
is given as
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x}  = 
 \left(
   \begin{array}[c]{l}
     2 \\ 1
   \end{array}
  \right)
$.
\\[0.2cm]
However, let us assume that the vector $\vec{b}$ is distorted into the vector $\vec{c}$ that is
given as
\\[0.2cm]
\hspace*{1.3cm}
$\vec{c} := \left(
  \begin{array}[c]{l}
    4.001 \\ 7.998
  \end{array}
  \right).
$
\\[0.2cm]
If we solve the system
\\[0.2cm]
\hspace*{1.3cm}
$\vec{c} = a \cdot \vec{x}$,
\\[0.2cm]
the solution for $\vec{x}$ becomes
\\[0.2cm]
\hspace*{1.3cm}
$\vec{x}  = 
 \left(
   \begin{array}[c]{r}
     -3.999 \\ 4.000
   \end{array}
  \right)
$.
\\[0.2cm]
We see that a tiny change in the right hand side of the system of equations has caused a big change
in the solution $\vec{x}$.  The reason is that the matrix \texttt{a} is \emph{ill-conditioned}.
We can verify this using the function \texttt{la\_cond}:  The expression
\\[0.2cm]
\hspace*{1.3cm}
\texttt{la\_cond(<< <<1.000 2.000>> <<2.000 3.999>> >>);}
\\[0.2cm]
yields the result 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{24992.000959995028}.
\\[0.2cm]
This shows that small errors in the right hand side of a system of linear equations involving the
matrix \texttt{a} are multiplied by a factor of nearly $25\,000$.  Our recommendation is that if you
ever have to solve a system of equations, you should first check whether the system is
well-conditioned by computing the condition number of the matrix associated with the system.  This 
condition number tells us by how much an error in the right hand side of the system of equations is
magnified when we compute the solution.  In particular, if the condition number is so big that it
will magnify known uncertainties in  the right hand side of the equations to an extend that the
result is meaningless, then it is not possible to solve the given system of equations in a meaningful
way. 


\subsection{The Singular Value Decomposition and the Pseudo-Inverse}
The function \texttt{la\_svd} can be used to compute the 
\href{http://en.wikipedia.org/wiki/Singular_value_decomposition}{\emph{singular value decomposition}} 
of a given matrix \texttt{a}.  For a given $m \times n$ matrix \texttt{a}, the expression 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{la\_svd(a)}
\\[0.2cm]
returns a list of the form
\\[0.2cm]
\hspace*{1.3cm}
\texttt{[u, s, v]}
\\[0.2cm]
where \texttt{u} is an 
\href{http://en.wikipedia.org/wiki/Orthogonal_matrix}{orthogonal} $m \times m$ matrix, \texttt{s}
is an $m \times n$ 
\href{http://en.wikipedia.org/wiki/Diagonal_matrix}{diagonal matrix}, and \texttt{v} is an $n \times n$
orthogonal matrix such that
\\[0.2cm]
\hspace*{1.3cm}
$a = u \cdot s \cdot v^{\mathtt{T}}$,
\\[0.2cm]
where $v^{\mathtt{T}}$ denotes the transpose of the matrix $v$.
  In practice, the singular value decomposition is used to compute the
\href{http://en.wikipedia.org/wiki/Moore-Penrose_pseudoinverse}{\emph{pseudo-inverse}}
 of a singular matrix.   However, this can be done directly as for a matrix \texttt{a} the call
\\[0.2cm]
\hspace*{1.3cm}
\texttt{la\_pseudoInverse(a);}
\\[0.2cm]
computes the pseudo-inverse of \texttt{a}.

\subsection{Eigenvalues and Eigenvectors}
In order to compute the 
\href{http://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors}{eigenvalues and eigenvectors} of a
square matrix, \setlx\ provides the functions
\\[0.2cm]
\hspace*{1.3cm}
\texttt{la\_eigenValues} \quad and \quad \texttt{la\_eigenVectors}.
\\[0.2cm]
Both of these function take a single argument \texttt{a}, where \texttt{a} must be a square matrix.
The function \texttt{la\_eigenValues} returns a list of the eigenvalues of \texttt{a}.  The
function call \texttt{la\_eigenVectors(a)} returns a list of the eigenvectors of the matrix \texttt{a}.
For example, in order to compute the eigenvalues and eigenvectors of the matrix
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{a} :=
\left(\begin{array}[c]{ll}
  1 & 2 \\
  2 & 3
\end{array}\right)
$
\\[0.2cm]
we can execute the following commands:

\begin{Verbatim}[ frame         = lines, 
                  framesep      = 0.3cm, 
                  firstnumber   = 1,
                  labelposition = bottomline,
                  numbers       = left,
                  numbersep     = -0.2cm,
                  xleftmargin   = 0.8cm,
                  xrightmargin  = 0.8cm,
                ]
    a        := << <<1 2>> <<2 3>> >>;
    [l1, l2] := la_eigenValues(a);
    [x1, x2] := la_eigenVectors(a);
\end{Verbatim}

Then, the eigenvalues will be computed as
\\[0.2cm]
\hspace*{1.3cm}
\texttt{l1 = -0.23606797749978958} \quad and \quad \texttt{l2 = 4.23606797749979}
\\[0.2cm]
In this case, the exact values of these eigenvalues are 
\\[0.2cm]
\hspace*{1.3cm}
$\mathtt{l1} = 2 - \sqrt{5}$ \quad and \quad $\mathtt{l2} = 2 + \sqrt{5}$.
\\[0.2cm]
The eigenvectors are computed as 
\\[0.2cm]
\hspace*{1.3cm}
\texttt{x1 = <<-0.8506508083520399 0.5257311121191336>>} \quad and \\
\hspace*{1.3cm}
\texttt{x2 = <<0.5257311121191336 0.8506508083520399>>}.
\\[0.2cm]
The eigenvectors computed are \emph{normalized}, that is the length of these vectors is $1$.
Furthermore, as the matrix \texttt{a} happens to be symmetric, i.e.~we have \texttt{a = a!},
the eigenvectors are orthogonal to each other.

Observe that not every square $n \times n$ matrix is 
\href{http://en.wikipedia.org/wiki/Diagonalizable_matrix}{\emph{diagonalizable}}
and hence has $n$ different eigenvectors.  In case the matrix is not diagonalizable in the real
numbers, the functions \texttt{la\_eigenValues} and \texttt{la\_eigenVectors} both throw an
exception.  Note, however, that every 
\href{http://en.wikipedia.org/wiki/Symmetric_matrix}{\emph{symmetric}} matrix is diagonalizable.
Therefore, if the matrix \texttt{a} is symmetric, neither of the two calls
\\[0.2cm]
\hspace*{1.3cm}
\texttt{la\_eigenValues(a)} \quad or \quad \texttt{la\_eigenVectors(a)}
\\[0.2cm]
will throw an exception.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "tutorial.tex"
%%% End: 



